{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evita rindani sembring\n",
    "### 12S15066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('collections/trec.sample.xml', 'r', encoding = 'utf-8')\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'HEAD' at 0x000001789A444D18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "tree = ElementTree()\n",
    "tree.parse(\"collections/trec.sample.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '3323', '3324', '3325', '3326', '3327', '3328', '3329', '3330', '3331', '3332', '3333', '3334', '3335', '3336', '3337', '3338', '3339', '3340', '3341', '3342', '3343', '3344', '3345', '3346', '3347', '3348', '3349', '3350', '3351', '3352', '3353', '3354', '3355', '3356', '3357', '3358', '3359', '3360', '3361', '3362', '3363', '3364', '3365', '3366', '3367', '3368', '3369', '3370', '3371', '3372', '3373', '3374', '3375', '3376', '3377', '3378', '3379', '3380', '3381', '3382', '3383', '3384', '3385', '3386', '3387', '3388', '3389', '3390', '3391', '3392', '3393', '3394', '3395', '3396', '3397', '3398', '3399', '3400', '3401', '3402', '3403', '3404', '3405', '3406', '3407', '3408', '3409', '3410', '3411', '3412', '3413', '3414', '3415', '3416', '3417', '3418', '3419', '3420', '3421', '3422', '3423', '3424', '3425', '3426', '3427', '3428', '3429', '3430', '3431', '3432', '3433', '3434', '3435', '3436', '3437', '3438', '3439', '3440', '3441', '3442', '3443', '3444', '3445', '3446', '3447', '3448', '3449', '3450', '3451', '3452', '3453', '3454', '3455', '3456', '3457', '3458', '3459', '3460', '3461', '3462', '3463', '3464', '3465', '3466', '3467', '3468', '3469', '3470', '3471', '3472', '3473', '3474', '3475', '3476', '3477', '3478', '3479', '3480', '3481', '3482', '3483', '3484', '3485', '3486', '3487', '3488', '3489', '3490', '3491', '3492', '3493', '3494', '3495', '3496', '3497', '3498', '3499', '3500', '3501', '3502', '3503', '3504', '3505', '3506', '3507', '3508', '3509', '3510', '3511', '3512', '3513', '3514', '3515', '3516', '3517', '3518', '3519', '3520', '3521', '3522', '3523', '3524', '3525', '3526', '3527', '3528', '3529', '3530', '3531', '3532', '3533', '3534', '3535', '3536', '3537', '3538', '3539', '3540', '3541', '3542', '3543', '3544', '3545', '3546', '3547', '3548', '3549', '3550', '3551', '3552', '3553', '3554', '3555', '3556', '3557', '3558', '3559', '3560', '3561', '3562', '3563', '3564', '3565', '3566', '3567', '3568', '3569', '3570', '3571', '3572', '3573', '3574', '3575', '3576', '3577', '3578', '3579', '3580', '3581', '3582', '3583', '3584', '3585', '3586', '3587', '3588', '3589', '3590', '3591', '3592', '3593', '3594', '3595', '3596', '3597', '3598', '3599', '3600', '3601', '3602', '3603', '3604', '3605', '3606', '3607', '3608', '3609', '3610', '3611', '3612', '3613', '3614', '3615', '3616', '3617', '3618', '3619', '3620', '3621', '3622', '3623', '3624', '3625', '3626', '3627', '3628', '3629', '3630', '3631', '3632', '3633', '3634', '3635', '3636', '3637', '3638', '3639', '3640', '3641', '3642', '3643', '3644', '3645', '3646', '3647', '3648', '3649', '3650', '3651', '3652', '3653', '3654', '3655', '3656', '3657', '3658', '3659', '3660', '3661', '3662', '3663', '3664', '3665', '3666', '3667', '3668', '3669', '3670', '3671', '3672', '3673', '3674', '3675', '3676', '3677', '3678', '3679', '3680', '3681', '3682', '3683', '3684', '3685', '3686', '3687', '3688', '3689', '3690', '3691', '3692', '3693', '3694', '3695', '3696', '3697', '3698', '3699', '3700', '3701', '3702', '3703', '3704', '3705', '3706', '3707', '3708', '3709', '3710', '3711', '3712', '3713', '3714', '3715', '3716', '3717', '3718', '3719', '3720', '3721', '3722', '3723', '3724', '3725', '3726', '3727', '3728', '3729', '3730', '3731', '3732', '3733', '3734', '3735', '3736', '3737', '3738', '3739', '3740', '3741', '3742', '3743', '3744', '3745', '3746', '3747', '3748', '3749', '3750', '3751', '3752', '3753', '3754', '3755', '3756', '3757', '3758', '3759', '3760', '3761', '3762', '3763', '3764', '3765', '3766', '3767', '3768', '3769', '3770', '3771', '3772', '3773', '3774', '3775', '3776', '3777', '3778', '3779', '3780', '3781', '3782', '3783', '3784', '3785', '3786', '3787', '3788', '3789', '3790', '3791', '3792', '3793', '3794', '3795', '3796', '3797', '3798', '3799', '3800', '3801', '3802', '3803', '3804', '3805', '3806', '3807', '3808', '3809', '3810', '3811', '3812', '3813', '3814', '3815', '3816', '3817', '3818', '3819', '3820', '3821', '3822', '3823', '3824', '3825', '3826', '3827', '3828', '3829', '3830', '3831', '3832', '3833', '3834', '3835', '3836', '3837', '3838', '3839', '3840', '3841', '3842', '3843', '3844', '3845', '3846', '3847', '3848', '3849', '3850', '3851', '3852', '3853', '3854', '3855', '3856', '3857', '3858', '3859', '3860', '3861', '3862', '3863', '3864', '3865', '3866', '3867', '3868', '3869', '3870', '3871', '3872', '3873', '3874', '3875', '3876', '3877', '3878', '3879', '3880', '3881', '3882', '3883', '3884', '3885', '3886', '3887', '3888', '3889', '3890', '3891', '3892', '3893', '3894', '3895', '3896', '3897', '3898', '3899', '3900', '3901', '3902', '3903', '3904', '3905', '3906', '3907', '3908', '3909', '3910', '3911', '3912', '3913', '3914', '3915', '3916', '3917', '3918', '3919', '3920', '3921', '3922', '3923', '3924', '3925', '3926', '3927', '3928', '3929', '3930', '3931', '3932', '3933', '3934', '3935', '3936', '3937', '3938', '3939', '3940', '3941', '3942', '3943', '3944', '3945', '3946', '3947']\n"
     ]
    }
   ],
   "source": [
    "docnum = []\n",
    "headline = []\n",
    "text = []\n",
    "\n",
    "for node in tree.iter(\"DOCNO\"):\n",
    "    docnum.append(node.text)\n",
    "    \n",
    "for node in tree.iter(\"HEADLINE\"):\n",
    "    headline.append(node.text)\n",
    "\n",
    "for node in tree.iter(\"TEXT\"):\n",
    "    text.append(node.text)\n",
    "    \n",
    "sums = []\n",
    "for i in range(len(text)):\n",
    "    sums.append(headline[i] + text[i])\n",
    "    \n",
    "sums[0]\n",
    "print(docnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = []\n",
    "translator = str.maketrans('','',string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sums)):\n",
    "    sums[i] = sums[i].translate(translator)\n",
    "    sums[i] = ''.join([x for x in sums[i] if not x.isdigit()])\n",
    "    sums[i] = re.sub('r ^ https ?:\\/\\/.*[\\r\\n]*','',sums[i],flags = re.MULTILINE)\n",
    "    tokenize.append(word_tokenize(sums[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenize)):\n",
    "    for word in tokenize[i]:\n",
    "        tokenize[i] = [word.lower() for word in tokenize[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "for i in range(len(tokenize)):\n",
    "    newList = [w for w in tokenize[i] if not w in stopwords.words('english')]\n",
    "    datalist.append(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "dataStemList = []\n",
    "unique = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datalist)):\n",
    "    news = []\n",
    "    for word in datalist[i]:\n",
    "        if(word != stemmer.stem(word)):\n",
    "            word = stemmer.stem(word)\n",
    "            news.append(word)\n",
    "        else:\n",
    "            news.append(word)\n",
    "        if not word in unique:\n",
    "            unique.append(word)\n",
    "            \n",
    "    dataStemList.append(news)\n",
    "    del news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = open('collections/queries.lab3.txt', 'r', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "for i in files.readlines():\n",
    "    datas.append(i)\n",
    "    \n",
    "query = []\n",
    "for x in datas:\n",
    "    w = x.split()\n",
    "    l = []\n",
    "    for word in w:\n",
    "        if not word.isdigit():\n",
    "            l.append(word)\n",
    "    query.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casefolding\n",
    "for i in range(len(query)):\n",
    "    query[i] = [word.lower() for word in query[i]]\n",
    "     \n",
    "#Stopping\n",
    "for i in range(len(query)):\n",
    "    query[i] = [w for w in query[i] if not w in stopwords.words('english')]\n",
    "    \n",
    "    \n",
    "#stemming\n",
    "for i in range(len(query)):\n",
    "    query[i] = [stemmer.stem(word) if word != stemmer.stem(word) else word for word in query[i]]\n",
    "    \n",
    "    \n",
    "uniq = []\n",
    "for i in range(len(query)):\n",
    "    for word in query[i]:\n",
    "        if not word in uniq:\n",
    "            uniq.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Implementation using <i>Term Weighting</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataStemList)\n",
    "df = []\n",
    "res = []\n",
    "wtd = []\n",
    "hasil = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    tot = 0\n",
    "    for j in range(len(dataStemList)):\n",
    "        if uniq[i] in dataStemList[j]:\n",
    "            tot += 1\n",
    "    df.append(tot)\n",
    "    \n",
    "for i in range(len(df)):\n",
    "    res.append(math.log10(N/df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    lists = []\n",
    "    for j in range(len(dataStemList)):\n",
    "        dicts = {}\n",
    "        x = dataStemList[j].count(uniq[i])\n",
    "        if x == 0:\n",
    "            dicts[j+1] = 0\n",
    "            lists.append(dicts)\n",
    "        else:\n",
    "            score = math.log10(x)\n",
    "            score += 1\n",
    "            score *= res[i]\n",
    "            dicts[j+1] = score\n",
    "            lists.append(dicts)\n",
    "    wtd.append(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    l = []\n",
    "    for j in range(len(dataStemList)):\n",
    "        dic = {}\n",
    "        for word in query[i]:\n",
    "            sums = 0\n",
    "            ind = uniq.index(word)\n",
    "            #print(ind)\n",
    "            for val in wtd[ind][j].values():\n",
    "                sums += val\n",
    "        if(sums!= 0):\n",
    "            dic['docnum'] = j+1\n",
    "            dic['score'] = sums\n",
    "        if(len(dic) != 0): l.append(dic)\n",
    "    hasil.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    hasil[i] = sorted(hasil[i], key = lambda x : x['score'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    with open('Tugas1A-{counter}.txt'.format(counter = i+1), 'w') as f:\n",
    "        f.write('First 10 Documents:\\n')\n",
    "        f.write('Q_ID | DOCNO | SCORE\\n')\n",
    "        if len(hasil[i]) > 10:\n",
    "            for x in range(10):\n",
    "                c = i + 1\n",
    "                f.write('%s   0   %s   0   %s\\n' %(c,docnum[hasil[i][x]['docnum']-1],hasil[i][x]['score']))\n",
    "        else:\n",
    "            for x in hasil[i]:\n",
    "                c  = i + 1\n",
    "                f.write('%s   0   %s   0   %s\\n' %(c,docnum[x['docnum']-1],x['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Implementation using <i>Smart Notation</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfq = []\n",
    "resq = []\n",
    "wtq = []\n",
    "new_wtd = []\n",
    "norm = []\n",
    "norms = []   \n",
    "rescosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    s = 0\n",
    "    for x in range(len(query)):\n",
    "        if uniq[i] in query[x]:\n",
    "            s += 1\n",
    "    dfq.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dfq)):\n",
    "    resq.append(math.log10(N / dfq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    lists = []\n",
    "    for j in range(len(query)):\n",
    "        dicts = {}\n",
    "        x = query[j].count(uniq[i])\n",
    "        if x == 0:\n",
    "            dicts[j+1] = 0\n",
    "            lists.append(dicts)\n",
    "        else:\n",
    "            score = math.log10(x)\n",
    "            score += 1\n",
    "            score *= resq[i]\n",
    "            dicts[j+1] = score\n",
    "            lists.append(dicts)\n",
    "    wtq.append(lists)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    lists = []\n",
    "    for j in range(len(dataStemList)):\n",
    "        dicts = {}\n",
    "        x = dataStemList[j].count(uniq[i])\n",
    "        if x == 0:\n",
    "            dicts[j+1] = 0\n",
    "            lists.append(dicts)\n",
    "        else:\n",
    "            score = math.log10(x)\n",
    "            score += 1\n",
    "            dicts[j+1] = score  \n",
    "            lists.append(dicts)\n",
    "    new_wtd.append(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    ss = []\n",
    "    g = 0\n",
    "    for x in wtq[i]:\n",
    "        for val in x.values():\n",
    "            if val!=0: ss.append(val)\n",
    "    #print(ss)\n",
    "    for c in ss:\n",
    "        g = g + math.pow(c,2)\n",
    "    norm.append(math.sqrt(g))\n",
    "\n",
    "\n",
    "for i in range(len(uniq)):\n",
    "    for x in wtq[i]:\n",
    "        for key,val in x.items():\n",
    "            val = val / norm[i]\n",
    "            x[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(uniq)):\n",
    "    ss = []\n",
    "    g = 0\n",
    "    for x in new_wtd[i]:\n",
    "        for val in x.values():\n",
    "            if val != 0: ss.append(val)\n",
    "    for c in ss:\n",
    "        g = g + math.pow(c,2)\n",
    "    norms.append(math.sqrt(g))\n",
    "    \n",
    "\n",
    "for i in range(len(uniq)):\n",
    "    for x in new_wtd[i]:\n",
    "        for key,val in x.items():\n",
    "            val = val / norms[i]\n",
    "            x[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    hasilcosine  = []\n",
    "    for j in range(len(dataStemList)):\n",
    "        dix  = {}\n",
    "        ans = []\n",
    "        for kata in query[i]:\n",
    "            ind = uniq.index(word)\n",
    "            for x,y in zip(wtq[ind][i].values(),new_wtd[ind][j].values()):\n",
    "                ans.append(x*y)\n",
    "        #print(i+1, j+1, sum(ans))\n",
    "        if sum(ans)!=0:\n",
    "            dix['docnum'] = j+1\n",
    "            dix['score'] = sum(ans)\n",
    "        if len(dix) != 0: hasilcosine.append(dix)\n",
    "    rescosine.append(hasilcosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    rescosine[i] = sorted(rescosine[i], key = lambda x : x['score'], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(query)):\n",
    "    with open('Tugas1B-{counter}.txt'.format(counter = i+1), 'w') as f:\n",
    "        f.write('First 10 Documents:\\n')\n",
    "        f.write('Q_ID | DOCNO | SCORE\\n')\n",
    "        if len(rescosine[i]) > 10:\n",
    "            for x in range(10):\n",
    "                c = i + 1\n",
    "                f.write('%s   0   %s   0   %s\\n' %(c,docnum[rescosine[i][x]['docnum']-1],rescosine[i][x]['score']))\n",
    "        else:\n",
    "            for x in hasil[i]:\n",
    "                c  = i + 1\n",
    "                f.write('%s   0   %s   0   %s\\n' %(c,docnum[x['docnum']-1],x['score']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
